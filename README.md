## DIFFUSION MULTIMODAL DISTILLATION COLLABORATION: A Generative  Equilibrium Framework for Efficient Vehicle Networking Intrusion  Detection

## æ‰©æ•£å¤šæ¨¡æ€è’¸é¦åä½œï¼šé¢å‘é«˜æ•ˆè½¦è”ç½‘å…¥ä¾µæ£€æµ‹çš„ç”Ÿæˆå¼å‡è¡¡æ¡†æ¶

## ğŸ—ï¸ GitHubä»“åº“ç»“æ„

text

```
IDS-Multimodal-DDPM/
â”œâ”€â”€ README.md                          # é¡¹ç›®ä¸»æ–‡æ¡£
â”œâ”€â”€ requirements.txt                   # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ environment.yml                    # Condaç¯å¢ƒé…ç½®
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ datasets.yaml                  # æ•°æ®é›†é…ç½®
â”‚   â”œâ”€â”€ experiments.yaml               # å®éªŒè¶…å‚æ•°
â”‚   â””â”€â”€ model_architectures.yaml       # æ¨¡å‹æ¶æ„é…ç½®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_preprocessing/            # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚   â”‚   â”œâ”€â”€ 01_feature_extraction.py
â”‚   â”‚   â”œâ”€â”€ 02_feature_grouping.py
â”‚   â”‚   â”œâ”€â”€ 03_image_generation.py
â”‚   â”‚   â””â”€â”€ 04_data_balancing.py
â”‚   â”œâ”€â”€ models/                        # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ train_multimodal.py
â”‚   â”‚   â”œâ”€â”€ train_baseline_models.py
â”‚   â”‚   â””â”€â”€ knowledge_distillation.py
â”‚   â””â”€â”€ evaluation/                    # è¯„ä¼°è„šæœ¬
â”‚       â”œâ”€â”€ evaluate_models.py
â”‚       â”œâ”€â”€ generate_results.py
â”‚       â””â”€â”€ statistical_tests.py
â”œâ”€â”€ src/                               # æºä»£ç 
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”‚   â””â”€â”€ transforms.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ multimodal.py
â”‚   â”‚   â”œâ”€â”€ ddpm.py
â”‚   â”‚   â”œâ”€â”€ baselines.py
â”‚   â”‚   â””â”€â”€ distillation.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ base_experiment.py
â”‚       â”œâ”€â”€ multimodal_experiment.py
â”‚       â””â”€â”€ baseline_experiment.py
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_analysis.ipynb
â”‚   â””â”€â”€ 03_result_analysis.ipynb
â”œâ”€â”€ tests/                             # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_utils.py
â””â”€â”€ docs/                              # æ–‡æ¡£
    â”œâ”€â”€ dataset_preprocessing.md
    â”œâ”€â”€ hyperparameter_documentation.md
    â””â”€â”€ reproduction_guide.md
```



## ğŸ“‹ æµç¨‹

### 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–ç®¡ç†

**requirements.txt:**

txt

```
torch>=1.9.0
torchvision>=0.10.0
scikit-learn>=0.24.0
pandas>=1.3.0
numpy>=1.21.0
opencv-python>=4.5.0
Pillow>=8.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
tqdm>=4.62.0
diffusers>=0.3.0
efficientnet-pytorch>=0.7.0
scipy>=1.7.0
joblib>=1.0.0
```



### 2. æ•°æ®é›†é¢„å¤„ç†è„šæœ¬

**scripts/data_preprocessing/01_feature_extraction.py:**

python

```
import argparse
import yaml
from src.data.preprocessing import DataPreprocessor

def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†ç‰¹å¾æå–')
    parser.add_argument('--dataset', type=str, required=True, 
                       choices=['cicids2017', 'cicids2018', 'toniot'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--config', type=str, default='config/datasets.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = DataPreprocessor(config[args.dataset])
    
    # æ‰§è¡Œé¢„å¤„ç†æµç¨‹
    print(f"å¼€å§‹å¤„ç† {args.dataset} æ•°æ®é›†...")
    features, labels = preprocessor.load_and_preprocess()
    
    # ä¿å­˜é¢„å¤„ç†ç»“æœ
    preprocessor.save_processed_data(features, labels)
    print("é¢„å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()
```



### 3. è¶…å‚æ•°æ–‡æ¡£

**docs/hyperparameter_documentation.md:**

markdown

```
## å¤šæ¨¡æ€æ¨¡å‹è¶…å‚æ•°

### æ•°æ®é¢„å¤„ç†
- `img_size`: 32 - ç”Ÿæˆçš„RGBå›¾åƒå°ºå¯¸
- `n_clusters`: 3 - ç‰¹å¾åˆ†ç»„èšç±»æ•°
- `feature_selection_threshold`: 75 - é€‰æ‹©çš„ç‰¹å¾æ•°é‡

### DDPMè®­ç»ƒå‚æ•°
- `ddpm_epochs`: 150 - DDPMè®­ç»ƒè½®æ•°
- `ddpm_target_count`: 2000 - æ¯ä¸ªå°‘æ•°ç±»ç›®æ ‡æ ·æœ¬æ•°
- `table_ddpm_samples`: 1200 - è¡¨æ ¼DDPMç”Ÿæˆæ ·æœ¬æ•°
- `learning_rate`: 2e-4 - å­¦ä¹ ç‡
- `num_train_timesteps`: 1000 - è®­ç»ƒæ—¶é—´æ­¥æ•°

### å¤šæ¨¡æ€è®­ç»ƒå‚æ•°
- `batch_size`: 64 - æ‰¹æ¬¡å¤§å°
- `teacher_epochs`: 20 - æ•™å¸ˆæ¨¡å‹è®­ç»ƒè½®æ•°
- `student_epochs`: 20 - å­¦ç”Ÿæ¨¡å‹è®­ç»ƒè½®æ•°
- `k_folds`: 5 - äº¤å‰éªŒè¯æŠ˜æ•°
- `temperature`: 3.0 - çŸ¥è¯†è’¸é¦æ¸©åº¦
- `alpha`: 0.5 - çŸ¥è¯†è’¸é¦æŸå¤±æƒé‡

### ä¼˜åŒ–å™¨å‚æ•°
- `optimizer`: AdamW
- `weight_decay`: 1e-5

### é€šç”¨å‚æ•°
- `img_size`: 224 - è¾“å…¥å›¾åƒå°ºå¯¸
- `batch_size`: 64 - æ‰¹æ¬¡å¤§å°
- `num_epochs`: 20 - è®­ç»ƒè½®æ•°
- `learning_rate`: 0.001 - å­¦ä¹ ç‡

### æ¨¡å‹ç‰¹å®šå‚æ•°
- åˆ†ç±»å™¨å¤´æ ¹æ®æ•°æ®é›†ç±»åˆ«æ•°è°ƒæ•´
```



### 4. ä¸»è¿è¡Œè„šæœ¬

**scripts/run_experiment.py:**

python

```
#!/usr/bin/env python3


import argparse
import yaml
import torch
import random
import numpy as np
from src.experiments.multimodal_experiment import MultimodalExperiment
from src.experiments.baseline_experiment import BaselineExperiment

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œå…¥ä¾µæ£€æµ‹å®éªŒ')
    parser.add_argument('--experiment', type=str, required=True,
                       choices=['multimodal', 'baseline', 'all'],
                       help='å®éªŒç±»å‹')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['cicids2017', 'cicids2018', 'toniot', 'all'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--config', type=str, default='config/experiments.yaml',
                       help='å®éªŒé…ç½®æ–‡ä»¶')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    print(f"å¼€å§‹å®éªŒ: {args.experiment}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"è®¾å¤‡: {args.device}")
    
    # è¿è¡Œå®éªŒ
    if args.experiment in ['multimodal', 'all']:
        multimodal_exp = MultimodalExperiment(config['multimodal'])
        multimodal_exp.run(args.dataset)
    
    if args.experiment in ['baseline', 'all']:
        baseline_exp = BaselineExperiment(config['baseline'])
        baseline_exp.run(args.dataset)

if __name__ == "__main__":
    main()
```



### 5. å¯é‡ç°æ€§é…ç½®

**config/experiments.yaml:**

yaml

```
multimodal:
  data_preprocessing:
    img_size: 32
    n_clusters: 3
    feature_selection:
      method: "mutual_information"
      n_features: 75
      n_groups: 3
    
  ddpm:
    image_ddpm:
      epochs: 100
      target_count: 2000
      batch_size: 32
      learning_rate: 2e-4
      timesteps: 1000
    
    table_ddpm:
      samples_per_class: 1200
      timesteps: 300
      learning_rate: 1e-3
      epochs: 30
  
  training:
    batch_size: 64
    teacher_epochs: 20
    student_epochs: 20
    k_folds: 5
    learning_rate: 0.001
    weight_decay: 1e-5
    
  knowledge_distillation:
    temperature: 3.0
    alpha: 0.5
    loss_weights:
      ce: 0.5
      kd: 0.5

baseline:
  models:
    - "mobilenetv3"
    - "shufflenet" 
    - "alexnet"
    - "efficientnet-lite"
    - "resnet50"
  
  training:
    img_size: 224
    batch_size: 64
    epochs: 20
    learning_rate: 0.001
    k_folds: 5
  
  evaluation:
    metrics:
      - "accuracy"
      - "precision"
      - "recall" 
      - "f1_score"
      - "inference_time"
      - "model_size"

seeds:
  data_splitting: 42
  model_initialization: 42
  training: 42
```



### 6. è¿è¡Œå‘½ä»¤ç¤ºä¾‹

bash

```
# 1. è®¾ç½®ç¯å¢ƒ
conda env create -f environment.yml
conda activate ids-multimodal

# 2. æ•°æ®é¢„å¤„ç†
python scripts/data_preprocessing/01_feature_extraction.py --dataset cicids2017
python scripts/data_preprocessing/02_feature_grouping.py --dataset cicids2017
python scripts/data_preprocessing/03_image_generation.py --dataset cicids2017

# 3. è¿è¡Œå¤šæ¨¡æ€å®éªŒ
python scripts/run_experiment.py --experiment multimodal --dataset cicids2017 --seed 42

# 4. è¿è¡ŒåŸºå‡†å®éªŒ
python scripts/run_experiment.py --experiment baseline --dataset cicids2017 --seed 42

# 5. ç”Ÿæˆæœ€ç»ˆç»“æœ
python scripts/evaluation/generate_results.py --dataset cicids2017 --output results/

```
