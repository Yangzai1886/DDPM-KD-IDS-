#!/usr/bin/env python3
import os
import sys
import yaml
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split, StratifiedKFold


sys.path.append(os.path.join(os.path.dirname(__file__), '../../src'))
from models.multimodal import MultiModalIDS, DistilledStudentModel
from data.datasets import MultiModalDataset
from training.trainers import MultimodalTrainer, DistillationTrainer

def setup_multimodal_experiment(config, dataset_name):
   

    data_dir = config['datasets'][dataset_name]['preprocessing']['final_data_dir']
    
    features = np.load(os.path.join(data_dir, 'features_balanced.npy'))
    labels = np.load(os.path.join(data_dir, 'labels_balanced.npy'))
    
    import joblib
    le = joblib.load(os.path.join(data_dir, 'label_encoder.joblib'))
    class_names = le.classes_
    num_classes = len(class_names)
    feature_dim = features.shape[1]
    
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"ç‰¹å¾ç»´åº¦: {feature_dim}, ç±»åˆ«æ•°: {num_classes}")
    print(f"ç±»åˆ«åç§°: {class_names}")
    
    return features, labels, num_classes, feature_dim, class_names, le

def main():
    parser = argparse.ArgumentParser(description='å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['cicids2017', 'cicids2018', 'toniot'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--config', type=str, default='../../config/experiments.yaml',
                       help='å®éªŒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-config', type=str, default='../../config/datasets.yaml',
                       help='æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--fold', type=int, default=0,
                       help='äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆ0è¡¨ç¤ºè¿è¡Œæ‰€æœ‰æŠ˜ï¼‰')
    parser.add_argument('--skip-teacher', action='store_true',
                       help='è·³è¿‡æ•™å¸ˆæ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--skip-distillation', action='store_true',
                       help='è·³è¿‡çŸ¥è¯†è’¸é¦')
    
    args = parser.parse_args()
    
  
    with open(args.config, 'r') as f:
        exp_config = yaml.safe_load(f)
    
    with open(args.data_config, 'r') as f:
        data_config = yaml.safe_load(f)
    
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
  
    features, labels, num_classes, feature_dim, class_names, le = setup_multimodal_experiment(
        data_config, args.dataset
    )
    

    from torchvision import transforms
    image_transform = transforms.Compose([
        transforms.Resize((exp_config['experiment_config']['img_size'], 
                          exp_config['experiment_config']['img_size'])),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
  
    image_dir = data_config['datasets'][args.dataset]['preprocessing']['final_data_dir']
    full_dataset = MultiModalDataset(
        image_dir=image_dir,
        features=features,
        labels=labels,
        label_encoder=le,
        transform=image_transform
    )
    
    valid_labels = full_dataset.get_valid_labels()
    

    k_folds = exp_config['experiment_config']['k_folds']
    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    folds_to_run = [args.fold] if args.fold > 0 else range(1, k_folds + 1)
    
    for fold in folds_to_run:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹ç¬¬ {fold} æŠ˜è®­ç»ƒ")
        print(f"{'='*60}")
        
        train_idx, test_idx = list(skf.split(range(len(full_dataset)), valid_labels))[fold-1]
        
      
        train_indices, val_indices = train_test_split(
            train_idx, test_size=0.1, stratify=valid_labels[train_idx], random_state=42
        )
        
        train_dataset = Subset(full_dataset, train_indices)
        val_dataset = Subset(full_dataset, val_indices)
        test_dataset = Subset(full_dataset, test_idx)
        
     
        batch_size = exp_config['experiment_config']['batch_size']
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)
        test_loader = DataLoader(test_dataset, batch_size=batch_size)
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
   
        train_config = {
            'model_save_dir': data_config['datasets'][args.dataset]['preprocessing']['model_save_dir'],
            'learning_rate': exp_config['experiment_config']['optimizer']['lr'],
            'weight_decay': exp_config['experiment_config']['optimizer']['weight_decay']
        }
        
 
        if not args.skip_teacher:
            print(f"\n--- è®­ç»ƒæ•™å¸ˆæ¨¡å‹ ---")
            teacher_model = MultiModalIDS(num_classes, feature_dim).to(device)
            
            teacher_trainer = MultimodalTrainer(
                teacher_model, train_loader, val_loader, train_config, args.dataset, fold
            )
            teacher_model = teacher_trainer.train(
                num_epochs=exp_config['experiment_config']['teacher_epochs']
            )
            

            teacher_params = teacher_model.get_parameter_count()
            print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°æ€»é‡: {teacher_params:.2f}M")
        else:

            model_path = os.path.join(
                train_config['model_save_dir'], 
                f'best_teacher_model_fold_{fold}.pth'
            )
            if os.path.exists(model_path):
                teacher_model = MultiModalIDS(num_classes, feature_dim).to(device)
                teacher_model.load_state_dict(torch.load(model_path))
                teacher_params = teacher_model.get_parameter_count()
                print(f"åŠ è½½é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå‚æ•°æ€»é‡: {teacher_params:.2f}M")
            else:
                print(f"é”™è¯¯: æ‰¾ä¸åˆ°é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ {model_path}")
                continue
        

        if not args.skip_distillation:
            print(f"\n--- çŸ¥è¯†è’¸é¦è®­ç»ƒ ---")
            student_model = DistilledStudentModel(
                num_classes, feature_dim, 
                image_size=exp_config['experiment_config']['img_size']
            ).to(device)
            
 
            student_params = student_model.get_parameter_count()
            print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°æ€»é‡: {student_params:.2f}M")
            print(f"å­¦ç”Ÿæ¨¡å‹æ˜¯æ•™å¸ˆæ¨¡å‹çš„ {student_params / teacher_params * 100:.2f}%")
            
            distillation_config = {
                'model_save_dir': train_config['model_save_dir'],
                'learning_rate': exp_config['experiment_config']['optimizer']['lr'],
                'temperature': exp_config['experiment_config']['temp'],
                'alpha': exp_config['experiment_config']['alpha']
            }
            
            distillation_trainer = DistillationTrainer(
                teacher_model, student_model, train_loader, val_loader, 
                distillation_config, args.dataset, fold
            )
            student_model = distillation_trainer.train(
                num_epochs=exp_config['experiment_config']['student_epochs']
            )
        
        print(f"\nâœ… ç¬¬ {fold} æŠ˜è®­ç»ƒå®Œæˆ")
    
    print(f"\nğŸ‰ æ‰€æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()